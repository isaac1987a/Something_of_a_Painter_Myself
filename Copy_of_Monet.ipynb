{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5eb853-63de-418a-aa67-ab72a439576b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4c5eb853-63de-418a-aa67-ab72a439576b",
    "outputId": "e33a40aa-fd91-4e14-8064-70ec898fb5e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.0)\n",
      "^C\n",
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.14)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.9.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.5.0)\n",
      "Requirement already satisfied: tensorflow-hub==0.13.0 in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.13.0) (1.26.2)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow-hub==0.13.0) (4.23.4)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2023.11.17)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.66.4)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.1.0)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!chmod 600 /root/.kaggle/kaggle.json\n",
    "!pip install --upgrade pip\n",
    "!pip install --upgrade kaggle pandas matplotlib scikit-learn tensorflow-hub==0.13.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8300029a-d6d2-4a90-a6c0-4947eda265b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19974c0f-b2d2-42ca-9bbc-7eb27100da3a",
   "metadata": {
    "id": "19974c0f-b2d2-42ca-9bbc-7eb27100da3a"
   },
   "source": [
    "# Project Goal and Overview\n",
    "The goal of this project is to create a Generative Adversarial Network (GAN) to convert images into the monet style.  To do this, I will use the 300 monet images to perform the training.  There are 300 Monets, and 7000+ photos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8558c137-5e9d-43ec-ac82-c1e5907c51ea",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8558c137-5e9d-43ec-ac82-c1e5907c51ea",
    "outputId": "593f18b4-f93b-450e-9a11-b0b2b25c8155"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 21:20:44.981398: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-06 21:20:44.981435: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-06 21:20:44.982372: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-06 21:20:44.987960: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import kaggle\n",
    "import zipfile\n",
    "import tensorflow.keras as keras\n",
    "from keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import math\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "import urllib\n",
    "import pickle\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import gzip, pickle\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "import urllib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import requests\n",
    "from os import getcwd\n",
    "import tarfile\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "autotune = tf.data.AUTOTUNE\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7c25ba-9683-4aaf-ac05-684d97ad1de3",
   "metadata": {
    "id": "ae7c25ba-9683-4aaf-ac05-684d97ad1de3"
   },
   "source": [
    "# Data Description\n",
    "The data is 2 sets of 256x256x3 color photos.  300 are monet paintings, and ~7000 are just ordinary photos.  Images are in a TFREC (Tensor Flow Record) format, which is convienient ofr easy batching.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeff25e-441b-4299-b3c9-7e5e04f5cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders as I did my work off kaggle.\n",
    "\n",
    "tmp_dir = getcwd() + \"/kaggle/tmp/\"\n",
    "input_dir = getcwd() + \"/kaggle/input/\"\n",
    "working_dir = getcwd() + \"/kaggle/working/\"\n",
    "if not os.path.exists(getcwd() + \"/kaggle\"):\n",
    "    os.makedirs(getcwd() + \"/kaggle\")\n",
    "if not os.path.exists(getcwd() + \"/kaggle/tmp\"):\n",
    "    os.makedirs(getcwd() + \"/kaggle/tmp\")\n",
    "if not os.path.exists(getcwd() + \"/kaggle/tmp/images\"):\n",
    "    os.makedirs(getcwd() + \"/kaggle/tmp/images\")\n",
    "if not os.path.exists(getcwd() + \"/kaggle/working\"):\n",
    "    os.makedirs(getcwd() + \"/kaggle/working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa50e0-6ac1-4b0c-b856-eacfbbc1ab7e",
   "metadata": {
    "id": "f2aa50e0-6ac1-4b0c-b856-eacfbbc1ab7e"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(getcwd() + \"kaggle/input/\"):\n",
    "    !kaggle competitions download -c 'gan-getting-started'\n",
    "    #!mkdir kaggle\n",
    "    #!mkdir kaggle/input\n",
    "    #!mkdir kaggle/input/gan-getting-started\n",
    "    #!unzip -qq gan-getting-started.zip -d kaggle/input/gan-getting-started\n",
    "    with zipfile.ZipFile('gan-getting-started.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall(f\"{input_dir}gan-getting-started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01775d7-070a-4c44-a317-df991bf30e49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b01775d7-070a-4c44-a317-df991bf30e49",
    "outputId": "4af74225-845f-4c5a-f6ea-9ae887dacf06"
   },
   "outputs": [],
   "source": [
    "\n",
    "def count_files_in_directory(directory_path):\n",
    "    # List all files in the directory\n",
    "    files = [f for f in os.listdir(directory_path) if os.path.isfile(\n",
    "        os.path.join(directory_path, f))]\n",
    "    # Count the number of files\n",
    "    file_count = len(files)\n",
    "    return file_count\n",
    "directory_path = f'{input_dir}gan-getting-started/monet_jpg'\n",
    "file_count = count_files_in_directory(directory_path)\n",
    "print(f'There are {file_count} Monet Images.')\n",
    "\n",
    "directory_path = f'{input_dir}gan-getting-started/photo_jpg'\n",
    "file_count = count_files_in_directory(directory_path)\n",
    "print(f'There are {file_count} Photo Images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fd350-710c-4662-affa-0c1317b00df6",
   "metadata": {},
   "source": [
    "# EDA\n",
    "I did some basic looking at the various images to see what I got.  I also looked at T-SNE and Color Density Plots of the 2 data sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fEOFQxH41eUh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "fEOFQxH41eUh",
    "outputId": "0055a4ec-5500-46c0-a439-72217edc65fb"
   },
   "outputs": [],
   "source": [
    "#Generated by Gemini AI but modified to actually work\n",
    "# Get a list of all the files in the Monet directory\n",
    "monet_files = os.listdir(f\"{input_dir}gan-getting-started/monet_jpg\")\n",
    "\n",
    "# Get a list of all the files in the photo directory\n",
    "photo_files = os.listdir(f\"{input_dir}gan-getting-started/photo_jpg\")\n",
    "\n",
    "# Select 20 random Monet images\n",
    "random_monet_files = random.sample(monet_files, 20)\n",
    "\n",
    "# Select 20 random photo images\n",
    "random_photo_files = random.sample(photo_files, 20)\n",
    "\n",
    "# Create a 10x2 grid of subplots\n",
    "fig, axes = plt.subplots(4, 10, figsize=(10, 4))\n",
    "print(\"20 Monet Images above 20 Photos\")\n",
    "\n",
    "# Plot the Monet images in the first column\n",
    "\n",
    "for i, filename in enumerate(random_monet_files):\n",
    "    img = mpimg.imread(os.path.join(f\"{input_dir}/gan-getting-started/monet_jpg\", filename))\n",
    "    axes[math.floor(i%2), math.floor(i/2)].imshow(img)\n",
    "    axes[math.floor(i%2), math.floor(i/2)].axis('off')\n",
    "\n",
    "## Plot the photo images in the second column\n",
    "for i, filename in enumerate(random_photo_files):\n",
    "    img = mpimg.imread(os.path.join(f\"{input_dir}gan-getting-started/photo_jpg\", filename))\n",
    "    axes[math.floor(i%2+2), math.floor(i/2)].imshow(img)\n",
    "    axes[math.floor(i%2+2), math.floor(i/2)].axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8uKwt0Opy1p7",
   "metadata": {
    "id": "8uKwt0Opy1p7"
   },
   "source": [
    "# Data Loading and Batch Sizes\n",
    "In order to perform faster data loading, I chose to load the tfrec files.  I seleted batch sizes to meet GPU memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "N48QEbG-xZRr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N48QEbG-xZRr",
    "outputId": "f5480fd7-cc54-4cc8-9fe2-22c57a857849"
   },
   "outputs": [],
   "source": [
    "def count_elements(dataset):\n",
    "    count = 0\n",
    "    for _ in dataset:\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def decode_image(image):\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  image = tf.image.resize(image, [256, 256])\n",
    "  return image\n",
    "\n",
    "def read_tfrecord(example):\n",
    "  features = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "  }\n",
    "  example = tf.io.parse_single_example(example, features)\n",
    "  image = decode_image(example['image'])\n",
    "  return image\n",
    "tfrecord_dir = f\"{input_dir}gan-getting-started/monet_tfrec\"\n",
    "tfrecord_files = [os.path.join(tfrecord_dir, f) for f in os.listdir(\n",
    "    tfrecord_dir) if f.endswith('.tfrec')]\n",
    "monet_images_dataset = tf.data.TFRecordDataset(\n",
    "    tfrecord_files).map(read_tfrecord)\n",
    "\n",
    "tfrecord_dir = f\"{input_dir}gan-getting-started/photo_tfrec\"\n",
    "tfrecord_files = [os.path.join(tfrecord_dir, f) for f in os.listdir(\n",
    "    tfrecord_dir) if f.endswith('.tfrec')]\n",
    "photo_images_dataset = tf.data.TFRecordDataset(\n",
    "    tfrecord_files).map(read_tfrecord)\n",
    "\n",
    "print(monet_images_dataset)\n",
    "print(f\"There are {count_elements(monet_images_dataset)} Monet images\")\n",
    "print(f\"There are {count_elements(photo_images_dataset)} Photo images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QFG3NTueKiN3",
   "metadata": {
    "id": "QFG3NTueKiN3"
   },
   "outputs": [],
   "source": [
    "# https://keras.io/examples/generative/cyclegan/\n",
    "# Define the standard image size.\n",
    "orig_img_size = (256, 256)\n",
    "# Size of the random crops to be used during training.\n",
    "input_img_size = (256, 256, 3)\n",
    "# Weights initializer for the layers.\n",
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "# Gamma initializer for instance normalization.\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return (img / 127.5) - 1.0\n",
    "def preprocess_train_image(img):\n",
    "    # Random flip\n",
    "    print(img)\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    # Resize to the original size first\n",
    "    img = tf.image.resize(img, [*orig_img_size])\n",
    "    # Random crop to 256X256\n",
    "    img = tf.image.random_crop(img, size=[*input_img_size])\n",
    "    # Normalize the pixel values in the range [-1, 1]\n",
    "    img = normalize_img(img)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633f3b8-5a7a-40c6-b90c-6de4c1447ebd",
   "metadata": {
    "id": "f633f3b8-5a7a-40c6-b90c-6de4c1447ebd"
   },
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "image_depth = 3\n",
    "batch_size = 1\n",
    "LR = 0.00012\n",
    "buffer_size = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "srur779GKzvp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srur779GKzvp",
    "outputId": "d2eb68e9-b031-4331-c540-d2c275bad351"
   },
   "outputs": [],
   "source": [
    "# prompt: split TFRecordDataset into a train and test set 80/20 split\n",
    "\n",
    "# Apply the preprocessing operations to the training data\n",
    "monet_train_dataset = (\n",
    "    monet_images_dataset.map(preprocess_train_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# Apply the preprocessing operations to the test data\n",
    "photo_train_dataset = (\n",
    "    photo_images_dataset.map(preprocess_train_image, num_parallel_calls=autotune)\n",
    "    .cache()\n",
    "    .shuffle(buffer_size)\n",
    "    .batch(batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "print(f'Photo Images Count {count_elements(photo_train_dataset)}')\n",
    "\n",
    "print(f'Monet Images Count {count_elements(monet_train_dataset)}')\n",
    "all_batches = []\n",
    "for batch in monet_train_dataset:\n",
    "    # Convert each batch to a NumPy array and append to the list\n",
    "    all_batches.append(batch.numpy())\n",
    "    break\n",
    "print(all_batches[0].shape)\n",
    "\n",
    "#for i, img in enumerate(monet_test_dataset.take(1)):\n",
    "#    #print(f\"Image {i}: {img}\")\n",
    " #   plt.imshow(img[0] * 127.5 + 127.5)\n",
    " #   plt.title(f\"Image {i}\")\n",
    "#    plt.axis('off')\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fIswnkG0GM5Q",
   "metadata": {
    "id": "fIswnkG0GM5Q"
   },
   "source": [
    "#  Starting off the GAN\n",
    "I used https://keras.io/examples/generative/cyclegan/ as a reference for the initial model build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a763bd-8f66-45b0-80d9-68cb43894bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Instance Normalization due to TFA compatability \n",
    "class InstanceNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-5, gamma_initializer='ones', beta_initializer='zeros'):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma_initializer = gamma_initializer\n",
    "        self.beta_initializer = beta_initializer\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer=self.gamma_initializer,\n",
    "                                     trainable=True)\n",
    "        self.beta = self.add_weight(shape=(input_shape[-1],),\n",
    "                                    initializer=self.beta_initializer,\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, variance = tf.nn.moments(inputs, axes=[1, 2], keepdims=True)\n",
    "        normalized = (inputs - mean) / tf.sqrt(variance + self.epsilon)\n",
    "        return self.gamma * normalized + self.beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YYC8gsIqLwVs",
   "metadata": {
    "id": "YYC8gsIqLwVs"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ReflectionPadding2D(layers.Layer):\n",
    "    \"\"\"Implements Reflection Padding as a layer.\n",
    "\n",
    "    Args:\n",
    "        padding(tuple): Amount of padding for the\n",
    "        spatial dimensions.\n",
    "\n",
    "    Returns:\n",
    "        A padded tensor with the same type as the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        padding_tensor = [\n",
    "            [0, 0],\n",
    "            [padding_height, padding_height],\n",
    "            [padding_width, padding_width],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
    "\n",
    "\n",
    "def residual_block(\n",
    "    x,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    x = ReflectionPadding2D()(input_tensor)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.add([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eM_st7BCL0xq",
   "metadata": {
    "id": "eM_st7BCL0xq"
   },
   "outputs": [],
   "source": [
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=None,\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
    "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
    "        x\n",
    "    )\n",
    "    x = InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final block\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    model = keras.models.Model(img_input, x, name=name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_8LNDSTWL4AH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_8LNDSTWL4AH",
    "outputId": "040920e4-54db-426a-90d3-988f1ae031ab"
   },
   "outputs": [],
   "source": [
    "def get_discriminator(\n",
    "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        (4, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(img_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(3):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "            )\n",
    "        else:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(1, 1),\n",
    "            )\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u5MsfOjeL434",
   "metadata": {
    "id": "u5MsfOjeL434"
   },
   "outputs": [],
   "source": [
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_G,\n",
    "        generator_F,\n",
    "        discriminator_X,\n",
    "        discriminator_Y,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gen_G = generator_G\n",
    "        self.gen_F = generator_F\n",
    "        self.disc_X = discriminator_X\n",
    "        self.disc_Y = discriminator_Y\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_G_optimizer,\n",
    "        gen_F_optimizer,\n",
    "        disc_X_optimizer,\n",
    "        disc_Y_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "    ):\n",
    "        super().compile()\n",
    "        self.gen_G_optimizer = gen_G_optimizer\n",
    "        self.gen_F_optimizer = gen_F_optimizer\n",
    "        self.disc_X_optimizer = disc_X_optimizer\n",
    "        self.disc_Y_optimizer = disc_Y_optimizer\n",
    "        self.generator_loss_fn = gen_loss_fn\n",
    "        self.discriminator_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is Horse and y is zebra\n",
    "        real_x, real_y = batch_data\n",
    "\n",
    "        # For CycleGAN, we need to calculate different\n",
    "        # kinds of losses for the generators and discriminators.\n",
    "        # We will perform the following steps here:\n",
    "        #\n",
    "        # 1. Pass real images through the generators and get the generated images\n",
    "        # 2. Pass the generated images back to the generators to check if we\n",
    "        #    can predict the original image from the generated image.\n",
    "        # 3. Do an identity mapping of the real images using the generators.\n",
    "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
    "        # 5. Calculate the generators total loss (adversarial + cycle + identity)\n",
    "        # 6. Calculate the discriminators loss\n",
    "        # 7. Update the weights of the generators\n",
    "        # 8. Update the weights of the discriminators\n",
    "        # 9. Return the losses in a dictionary\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Horse to fake zebra\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            # Zebra to fake horse -> y2x\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator adversarial loss\n",
    "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
    "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
    "\n",
    "            # Generator cycle loss\n",
    "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
    "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
    "\n",
    "            # Generator identity loss\n",
    "            id_loss_G = (\n",
    "                self.identity_loss_fn(real_y, same_y)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "            id_loss_F = (\n",
    "                self.identity_loss_fn(real_x, same_x)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "\n",
    "            # Total generator loss\n",
    "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
    "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
    "\n",
    "            # Discriminator loss\n",
    "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Get the gradients for the generators\n",
    "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
    "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
    "\n",
    "        # Get the gradients for the discriminators\n",
    "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Update the weights of the generators\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(grads_G, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(grads_F, self.gen_F.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update the weights of the discriminators\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"G_loss\": total_loss_G,\n",
    "            \"F_loss\": total_loss_F,\n",
    "            \"D_X_loss\": disc_X_loss,\n",
    "            \"D_Y_loss\": disc_Y_loss,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "I3xKpMnzL3oC",
   "metadata": {
    "id": "I3xKpMnzL3oC"
   },
   "source": [
    "Gan Monitor prints out 4 pictures for visual feedback every 25 Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MQWl-RNhL3Pj",
   "metadata": {
    "id": "MQWl-RNhL3Pj"
   },
   "outputs": [],
   "source": [
    "run = 0\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
    "\n",
    "    def __init__(self, num_img=4):\n",
    "        self.num_img = num_img\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 25 != 0:\n",
    "            return\n",
    "        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n",
    "        for i, img in enumerate(photo_train_dataset.take(self.num_img)):\n",
    "            prediction = self.model.gen_G(img)[0].numpy()\n",
    "            prediction = (((prediction * 127.5) + 127.5) * 255).astype(np.uint8)\n",
    "            img = (((img[0] * 127.5) + 127.5) * 255).numpy().astype(np.uint8)\n",
    "\n",
    "            ax[i, 0].imshow(img)\n",
    "            ax[i, 1].imshow(prediction)\n",
    "            ax[i, 0].set_title(\"Input image\")\n",
    "            ax[i, 1].set_title(\"Translated image\")\n",
    "            ax[i, 0].axis(\"off\")\n",
    "            ax[i, 1].axis(\"off\")\n",
    "\n",
    "            prediction = keras.utils.array_to_img(prediction)\n",
    "            prediction.save(\n",
    "                \"{tmp_dir}generated_img_{i}_{epoch}_{run}.png\".format(i=i, epoch=epoch + 1, run = run, tmp_dir = tmp_dir)\n",
    "            )\n",
    "        plt.show()\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BiVJoh35L6Y9",
   "metadata": {
    "id": "BiVJoh35L6Y9"
   },
   "source": [
    "# Scoring Code\n",
    "Copied and modified from https://wandb.ai/ayush-thakur/gan-evaluation/reports/How-to-Evaluate-GANs-using-Frechet-Inception-Distance-FID---Vmlldzo0MTAxOTI.  \n",
    "\n",
    "This calculates the FID score and will be used in callbacks. I tried for the Modieifed Fid used by Kaggle but could not get it to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d4c664-8685-44f7-a269-44e5f031cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified version of https://www.kaggle.com/code/wendykan/demo-mifid-metric-for-dog-image-generation-comp\n",
    "#Modified to be TF 2 compatable and remove for loops with tensorflow.  \n",
    "#Heavily leaned on ChatGPT for quick modifications and packing into a callback\n",
    "vars = {}\n",
    "def save_images_jpg(num_img):\n",
    "    for i, img in enumerate(photo_train_dataset.take(num_img)):\n",
    "        prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()\n",
    "        prediction = ((prediction +1 )  * 127.5 *255).astype(np.uint8)\n",
    "        im = Image.fromarray(prediction)\n",
    "        im.save(f'{tmp_dir}images/image{i}.jpg')\n",
    "    \n",
    "class KernelEvalException(Exception):\n",
    "    pass\n",
    "\n",
    "class mifid_score_callback(keras.callbacks.Callback):\n",
    "    def __init__(self):\n",
    "        self.model_params = {\n",
    "            'Inception': {\n",
    "                'name': 'Inception', \n",
    "                'imsize': 256,\n",
    "                'output_layer': 'global_average_pooling2d',\n",
    "                'input_layer': 'input_1',\n",
    "                'output_shape': 2048,\n",
    "                'cosine_distance_eps': 0.1\n",
    "            }\n",
    "        }\n",
    "    def create_model(self):\n",
    "        \"\"\"Creates the InceptionV3 model from tf.keras.applications.\"\"\"\n",
    "        base_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                       pooling='avg',\n",
    "                                                       input_shape=(self.model_params['Inception']['imsize'],\n",
    "                                                                    self.model_params['Inception']['imsize'], 3))\n",
    "        model = tf.keras.Model(inputs=base_model.input, outputs=base_model.output)\n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def get_activations(self, images, model, batch_size=50, verbose=False):\n",
    "        \"\"\"Calculates the activations of the avg_pool layer for all images.\"\"\"\n",
    "        # Preprocess all images at once\n",
    "        images = tf.keras.applications.inception_v3.preprocess_input(images)\n",
    "        \n",
    "        # Predict all images at once\n",
    "        pred_arr = model.predict(images, batch_size=batch_size, verbose=verbose)\n",
    "        \n",
    "        # Reshape the prediction array if necessary\n",
    "        pred_arr = pred_arr.reshape(-1, self.model_params['Inception']['output_shape'])\n",
    "        \n",
    "        return pred_arr\n",
    "    \n",
    "    def normalize_rows(self, x: np.ndarray):\n",
    "        \"\"\"\n",
    "        Function that normalizes each row of the matrix x to have unit length.\n",
    "        \"\"\"\n",
    "        return np.nan_to_num(x / np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n",
    "    \n",
    "    def cosine_distance(self, features1, features2):\n",
    "        features1_nozero = features1[np.sum(features1, axis=1) != 0]\n",
    "        features2_nozero = features2[np.sum(features2, axis=1) != 0]\n",
    "        norm_f1 = self.normalize_rows(features1_nozero)\n",
    "        norm_f2 = self.normalize_rows(features2_nozero)\n",
    "    \n",
    "        d = 1.0 - np.abs(np.matmul(norm_f1, norm_f2.T))\n",
    "        print('d.shape=', d.shape)\n",
    "        print('np.min(d, axis=1).shape=', np.min(d, axis=1).shape)\n",
    "        mean_min_d = np.mean(np.min(d, axis=1))\n",
    "        print('distance=', mean_min_d)\n",
    "        return mean_min_d\n",
    "    \n",
    "    def distance_thresholding(self, d, eps):\n",
    "        if d < eps:\n",
    "            return d\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def calculate_frechet_distance(self,mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "        \"\"\"Numpy implementation of the Frechet Distance.\"\"\"\n",
    "        mu1 = np.atleast_1d(mu1)\n",
    "        mu2 = np.atleast_1d(mu2)\n",
    "    \n",
    "        sigma1 = np.atleast_2d(sigma1)\n",
    "        sigma2 = np.atleast_2d(sigma2)\n",
    "    \n",
    "        assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "        assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "    \n",
    "        diff = mu1 - mu2\n",
    "    \n",
    "        # product might be almost singular\n",
    "        covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "        if not np.isfinite(covmean).all():\n",
    "            msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "            warnings.warn(msg)\n",
    "            offset = np.eye(sigma1.shape[0]) * eps\n",
    "            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "        \n",
    "        # numerical error might give slight imaginary component\n",
    "        if np.iscomplexobj(covmean):\n",
    "            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "                m = np.max(np.abs(covmean.imag))\n",
    "                raise ValueError(\"Imaginary component {}\".format(m))\n",
    "            covmean = covmean.real\n",
    "    \n",
    "        print('covmean.shape=', covmean.shape)\n",
    "    \n",
    "        tr_covmean = np.trace(covmean)\n",
    "        return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "    \n",
    "    def calculate_activation_statistics(self,images, model, batch_size=50, verbose=False):\n",
    "        \"\"\"Calculation of the statistics used by the FID.\"\"\"\n",
    "        act = self.get_activations(images, model, batch_size, verbose)\n",
    "        mu = np.mean(act, axis=0)\n",
    "        sigma = np.cov(act, rowvar=False)\n",
    "        return mu, sigma, act\n",
    "    \n",
    "    def _handle_path_memorization(self,path, model, is_checksize, is_check_png):\n",
    "        path = pathlib.Path(path)\n",
    "        files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
    "        imsize = self.model_params['Inception']['imsize']\n",
    "        x = np.array([np.array(self.img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n",
    "        m, s, features =self.calculate_activation_statistics(x, model)\n",
    "        del x  # clean up memory\n",
    "        return m, s, features\n",
    "    \n",
    "    def img_read_checks(self,filename, resize_to, is_checksize=False, check_imsize=299, is_check_png=False):\n",
    "        im = Image.open(str(filename))\n",
    "        if is_checksize and im.size != (check_imsize, check_imsize):\n",
    "            raise KernelEvalException('The images are not of size ' + str(check_imsize))\n",
    "        if is_check_png and not (im.format == 'PNG' or im.format == 'JPEG'):\n",
    "            raise KernelEvalException('Only PNG or JPG images should be submitted.')\n",
    "    \n",
    "        if resize_to is None:\n",
    "            return im\n",
    "        else:\n",
    "            return im.resize((resize_to, resize_to), Image.LANCZOS)\n",
    "    \n",
    "    def calculate_kid_given_paths(self,paths, model_name, feature_path=None):\n",
    "        ''' Calculates the KID of two paths. '''\n",
    "        model = self.create_model()\n",
    "        m1, s1, features1 = self._handle_path_memorization(paths[0], model, is_checksize=True, is_check_png=True)\n",
    "        if feature_path is None:\n",
    "            m2, s2, features2 = self._handle_path_memorization(paths[1], model, is_checksize=False, is_check_png=False)\n",
    "        else:\n",
    "            with np.load(feature_path) as f:\n",
    "                m2, s2, features2 = f['m'], f['s'], f['features']\n",
    "    \n",
    "        print('m1,m2 shape=', (m1.shape, m2.shape), 's1,s2=', (s1.shape, s2.shape))\n",
    "        print('starting calculating FID')\n",
    "        fid_value = self.calculate_frechet_distance(m1, s1, m2, s2)\n",
    "        print('done with FID, starting distance calculation')\n",
    "        distance = self.cosine_distance(features1, features2)        \n",
    "        return fid_value, distance\n",
    "\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % 25 != 0:\n",
    "            return\n",
    "        output_file = f'{working_dir}results.csv'\n",
    "        if not os.path.exists(output_file):\n",
    "            output_data = pd.DataFrame(columns = list(vars.keys()))\n",
    "        else:\n",
    "            output_data = pd.read_csv(f'{working_dir}results.csv')\n",
    "        save_images_jpg(2000)\n",
    "        user_images_unzipped_path = f'{tmp_dir}images/'\n",
    "        images_path = [user_images_unzipped_path, f'{input_dir}gan-getting-started/monet_jpg/']\n",
    "        \n",
    "        fid_epsilon = 10e-15\n",
    "        outputline = vars\n",
    "        fid_value_public, distance_public = self.calculate_kid_given_paths(images_path, 'Inception')\n",
    "        distance_public = self.distance_thresholding(distance_public, self.model_params['Inception']['cosine_distance_eps'])\n",
    "        print(\"MIFID_public: \", fid_value_public, \"Distance Public: \", distance_public, \"multiplied_public: \", fid_value_public / (distance_public + fid_epsilon))\n",
    "        outputline.update({'MIFID_public':fid_value_public,\n",
    "                           'dist_public': distance_public,\n",
    "                           'mult_public':fid_value_public / (distance_public + fid_epsilon)})\n",
    "        output_data = pd.concat([output_data,pd.DataFrame(outputline, index=[0])])\n",
    "        output_data.to_csv(f'{working_dir}results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RIFd5rFhL_wH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RIFd5rFhL_wH",
    "outputId": "56b576dd-1c5e-4a3c-a05d-ff27f64348e6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss function for evaluating adversarial loss\n",
    "vars = {'gen_filters':64,\n",
    "'gen_num_downsampling_blocks':2,\n",
    "'gen_num_residual_blocks':9,\n",
    "'gen_num_upsample_blocks':2,\n",
    "'disc_filters':64,\n",
    "'disc_num_downsampling':3,\n",
    "'MIFID_public':0.0,\n",
    "'dist_public': 0.0,\n",
    " 'mult_public':0.0\n",
    "   }\n",
    "\n",
    "\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return tf.reduce_mean(fake_loss)\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(filters=vars['gen_filters'],\n",
    "    num_downsampling_blocks= vars['gen_num_downsampling_blocks'],\n",
    "    num_residual_blocks= vars['gen_num_upsample_blocks'],\n",
    "    num_upsample_blocks= vars['gen_num_upsample_blocks'],\n",
    "    name=\"generator_G\")\n",
    "gen_F =  get_resnet_generator(filters=vars['gen_filters'],\n",
    "    num_downsampling_blocks= vars['gen_num_downsampling_blocks'],\n",
    "    num_residual_blocks= vars['gen_num_upsample_blocks'],\n",
    "    num_upsample_blocks= vars['gen_num_upsample_blocks'],\n",
    "    name=\"generator_F\")\n",
    "\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(filters=vars['disc_filters'],\n",
    "                           num_downsampling=vars['disc_num_downsampling'],\n",
    "                           name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(filters=vars['disc_filters'],\n",
    "                           num_downsampling=vars['disc_num_downsampling'],\n",
    "                           name=\"discriminator_Y\")\n",
    "\n",
    "# Create cycle gan model\n",
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_F_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_X_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_Y_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn\n",
    ")\n",
    "mifid_score_cb = mifid_score_callback()\n",
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True\n",
    ")\n",
    "\n",
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((photo_train_dataset, monet_train_dataset)),\n",
    "    epochs=100,\n",
    "    callbacks=[plotter, model_checkpoint_callback, mifid_score_cb],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcae952-f724-4713-8fa8-eccb15ad0dcc",
   "metadata": {},
   "source": [
    "# Approach\n",
    "Now that I have gotten everyting working, I'm going to begin hyperparameter tuning.  I will be using 50 epochs to train.  I will be tuning the number of filers, and the number of up/down and residual blocks, along with tuning my "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dee3ee-da74-4d85-9f13-17715feaed60",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "92luZ7luMB2x",
    "outputId": "1e24f7f1-48d3-4efb-c614-01a6a55804f9"
   },
   "source": [
    "# Hyperparameter Tuning\n",
    "I chose to increase Hyperparameters by 50% 2 times and see what happens.  I hope I dont overload my GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010f4a3b-931b-493e-874f-db29980334df",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 1\n",
    "vars = {'gen_filters':96,\n",
    "    'gen_num_downsampling_blocks':3,\n",
    "    'gen_num_residual_blocks':15,\n",
    "    'gen_num_upsample_blocks':3,\n",
    "    'gamma_initializer': gamma_init,\n",
    "    'disc_filters':96,\n",
    "    'kernel_initializer':kernel_init, \n",
    "    'disc_num_downsampling':5,\n",
    "    'MIFID_public':0.0,\n",
    "    'dist_public': 0.0,\n",
    "     'mult_public':0.0\n",
    "       }\n",
    "\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return tf.reduce_mean(fake_loss)\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(filters=vars['gen_filters'],\n",
    "    num_downsampling_blocks= vars['gen_num_downsampling_blocks'],\n",
    "    num_residual_blocks= vars['gen_num_upsample_blocks'],\n",
    "    num_upsample_blocks= vars['gen_num_upsample_blocks'],\n",
    "    gamma_initializer= vars['gamma_initializer'],\n",
    "    name=\"generator_G\")\n",
    "gen_F =  get_resnet_generator(filters=vars['gen_filters'],\n",
    "    num_downsampling_blocks= vars['gen_num_downsampling_blocks'],\n",
    "    num_residual_blocks= vars['gen_num_upsample_blocks'],\n",
    "    num_upsample_blocks= vars['gen_num_upsample_blocks'],\n",
    "    gamma_initializer= vars['gamma_initializer'],\n",
    "    name=\"generator_F\")\n",
    "\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(filters=vars['disc_filters'],\n",
    "                           kernel_initializer=vars['kernel_initializer'], \n",
    "                           num_downsampling=vars['disc_num_downsampling'],\n",
    "                           name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(filters=vars['disc_filters'],\n",
    "                           kernel_initializer=vars['kernel_initializer'], \n",
    "                           num_downsampling=vars['disc_num_downsampling'],\n",
    "                           name=\"discriminator_Y\")\n",
    "\n",
    "# Create cycle gan model\n",
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_F_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_X_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_Y_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn\n",
    ")\n",
    "mifid_score_cb = mifid_score_callback()\n",
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True\n",
    ")\n",
    "\n",
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((photo_train_dataset, monet_train_dataset)),\n",
    "    epochs=100,\n",
    "    callbacks=[plotter, model_checkpoint_callback,mifid_score_cb],\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9603ba8-ea68-4ac3-859c-8cb2794cfeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 2\n",
    "\n",
    "# Loss function for evaluating adversarial loss\n",
    "vars = {'gen_filters':128,\n",
    "'gen_num_downsampling_blocks':4,\n",
    "'gen_num_residual_blocks':18,\n",
    "'gen_num_upsample_blocks':4,\n",
    "'gamma_initializer': gamma_init,\n",
    "'disc_filters':128,\n",
    "'kernel_initializer':kernel_init, \n",
    "'disc_num_downsampling':6,\n",
    "'MIFID_public':0.0,\n",
    "'dist_public': 0.0,\n",
    " 'mult_public':0.0\n",
    "   }\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return tf.reduce_mean(fake_loss)\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(filters=vars['gen_filters'],\n",
    "    num_downsampling_blocks= vars['gen_num_downsampling_blocks'],\n",
    "    num_residual_blocks= vars['gen_num_upsample_blocks'],\n",
    "    num_upsample_blocks= vars['gen_num_upsample_blocks'],\n",
    "    gamma_initializer= vars['gamma_initializer'],\n",
    "    name=\"generator_G\")\n",
    "gen_F =  get_resnet_generator(filters=vars['gen_filters'],\n",
    "    num_downsampling_blocks= vars['gen_num_downsampling_blocks'],\n",
    "    num_residual_blocks= vars['gen_num_upsample_blocks'],\n",
    "    num_upsample_blocks= vars['gen_num_upsample_blocks'],\n",
    "    gamma_initializer= vars['gamma_initializer'],\n",
    "    name=\"generator_F\")\n",
    "\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(filters=vars['disc_filters'],\n",
    "                           kernel_initializer=vars['kernel_initializer'], \n",
    "                           num_downsampling=vars['disc_num_downsampling'],\n",
    "                           name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(filters=vars['disc_filters'],\n",
    "                           kernel_initializer=vars['kernel_initializer'], \n",
    "                           num_downsampling=vars['disc_num_downsampling'],\n",
    "                           name=\"discriminator_Y\")\n",
    "\n",
    "# Create cycle gan model\n",
    "cycle_gan_model = CycleGan(\n",
    "    generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "cycle_gan_model.compile(\n",
    "    gen_G_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_F_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_X_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    disc_Y_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "    gen_loss_fn=generator_loss_fn,\n",
    "    disc_loss_fn=discriminator_loss_fn\n",
    ")\n",
    "mifid_score_cb = mifid_score_callback()\n",
    "plotter = GANMonitor()\n",
    "checkpoint_filepath = \"./model_checkpoints/cyclegan_checkpoints.{epoch:03d}.weights.h5\"\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath, save_weights_only=True\n",
    ")\n",
    "\n",
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((photo_train_dataset, monet_train_dataset)),\n",
    "    epochs=100,\n",
    "    callbacks=[plotter, model_checkpoint_callback,mifid_score_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3246664f-691d-4237-890d-72671f74b6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 2e7764a] no\n",
      " 3 files changed, 3146 insertions(+), 3157 deletions(-)\n",
      " rewrite .ipynb_checkpoints/Copy_of_Monet-checkpoint.ipynb (83%)\n",
      " rewrite Copy_of_Monet.ipynb (83%)\n",
      " delete mode 100644 kaggle/working/results.csv\n",
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 128 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 419.19 KiB | 11.03 MiB/s, done.\n",
      "Total 4 (delta 1), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n",
      "To github.com:isaac1987a/Something_of_a_Painter_Myself.git\n",
      "   75d5fda..2e7764a  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\"\n",
    "!git commit -m \"no\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc7cad9-fb23-47fd-8756-db59b8e6545d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213b0926-20c7-4751-943f-30aa92f6a580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2988be6d-b861-43b5-9557-0c7cf3073663",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0a4f2d-8fd9-4221-bd1d-e7a598f7528b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28653aef-c531-4993-92fe-57a4a5761bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
