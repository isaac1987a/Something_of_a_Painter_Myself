{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isaac1987a/Something_of_a_Painter_Myself/blob/main/Copy_of_Monet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5eb853-63de-418a-aa67-ab72a439576b",
      "metadata": {
        "id": "4c5eb853-63de-418a-aa67-ab72a439576b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc842e6c-0766-4ee5-f2d7-e5e3b54bf1be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow[and-cuda] in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: e in /usr/local/lib/python3.10/dist-packages (1.4.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.64.0)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (3.3.3)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (0.37.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (1.25.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.3.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.3.4.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.3.101 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.3.101)\n",
            "Requirement already satisfied: nvidia-cuda-nvcc-cu12==12.3.107 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.3.107)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.3.107 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.3.107)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.3.101 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.3.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.7.29 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (8.9.7.29)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.12.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (11.0.12.1)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.4.107 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (10.3.4.107)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.5.4.101 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (11.5.4.101)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.2.0.103 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.2.0.103)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.3.101 in /usr/local/lib/python3.10/dist-packages (from tensorflow[and-cuda]) (12.3.101)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow[and-cuda]) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow[and-cuda]) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.0.0->tensorflow[and-cuda]) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow[and-cuda]) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow[and-cuda]) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.0.0->tensorflow[and-cuda]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow[and-cuda]) (0.1.2)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Collecting mediapipe-model-maker\n",
            "  Downloading mediapipe_model_maker-0.2.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.4.0)\n",
            "Collecting mediapipe>=0.10.0 (from mediapipe-model-maker)\n",
            "  Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (1.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mediapipe-model-maker) (4.9.0.80)\n",
            "Collecting tensorflow<2.16,>=2.10 (from mediapipe-model-maker)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if not os.path.exists(\"/root/.kaggle/\"):\n",
        "    os.makedirs(\"/root/.kaggle/\")\n",
        "    with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "      # Write the text to the file\n",
        "      f.write(\"\"\"{\"username\":\"isaacblach\",\"key\":\n",
        "        \"599d02c69c483f5ead7487a5b0bbae5b\"}\"\"\")\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!pip install tensorflow-addons"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19974c0f-b2d2-42ca-9bbc-7eb27100da3a",
      "metadata": {
        "id": "19974c0f-b2d2-42ca-9bbc-7eb27100da3a"
      },
      "source": [
        "# Project Goal and Overview\n",
        "The goal of this project is to create a Generative Adversarial Network (GAN) to convert images into the monet style.  To do this, I will use the 300 monet images to perform the training.  There are 300 Monets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8558c137-5e9d-43ec-ac82-c1e5907c51ea",
      "metadata": {
        "id": "8558c137-5e9d-43ec-ac82-c1e5907c51ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "96411c78-7246-4cca-a3ff-bfe54c9bea7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.13.0 and strictly below 2.16.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.16.1 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.src.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-05f4bd00af0f>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mautotune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Local project imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Additional activation functions.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgelu\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhardshrink\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhardshrink\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlisht\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlisht\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/activations/gelu.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_addons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTensorLike\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# New versions of Keras require importing from `keras.src` when\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# importing internal symbols.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2.5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.src.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import kaggle\n",
        "import zipfile\n",
        "import keras\n",
        "from keras import layers\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import matplotlib.image as mpimg\n",
        "import random\n",
        "import math\n",
        "import tensorflow_addons as tfa\n",
        "autotune = tf.data.AUTOTUNE\n",
        "print(tf.__version__)\n",
        "print(tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2aa50e0-6ac1-4b0c-b856-eacfbbc1ab7e",
      "metadata": {
        "id": "f2aa50e0-6ac1-4b0c-b856-eacfbbc1ab7e"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists('kaggle'):\n",
        "    !kaggle competitions download -c 'gan-getting-started'\n",
        "    #!mkdir kaggle\n",
        "    #!mkdir kaggle/input\n",
        "    #!mkdir kaggle/input/gan-getting-started\n",
        "    #!unzip -qq gan-getting-started.zip -d kaggle/input/gan-getting-started\n",
        "    with zipfile.ZipFile('gan-getting-started.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('./kaggle/input/gan-getting-started')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae7c25ba-9683-4aaf-ac05-684d97ad1de3",
      "metadata": {
        "id": "ae7c25ba-9683-4aaf-ac05-684d97ad1de3"
      },
      "source": [
        "# Data Description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b01775d7-070a-4c44-a317-df991bf30e49",
      "metadata": {
        "id": "b01775d7-070a-4c44-a317-df991bf30e49"
      },
      "outputs": [],
      "source": [
        "def count_files_in_directory(directory_path):\n",
        "    # List all files in the directory\n",
        "    files = [f for f in os.listdir(directory_path) if os.path.isfile(\n",
        "        os.path.join(directory_path, f))]\n",
        "    # Count the number of files\n",
        "    file_count = len(files)\n",
        "    return file_count\n",
        "directory_path = './kaggle/input/gan-getting-started/monet_jpg'\n",
        "file_count = count_files_in_directory(directory_path)\n",
        "print(f'There are {file_count} Monet Images.')\n",
        "\n",
        "directory_path = './kaggle/input/gan-getting-started/photo_jpg'\n",
        "file_count = count_files_in_directory(directory_path)\n",
        "print(f'There are {file_count} Monet Images.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generated by Gemini AI but modified to actually work\n",
        "# Get a list of all the files in the Monet directory\n",
        "monet_files = os.listdir(\"./kaggle/input/gan-getting-started/monet_jpg\")\n",
        "\n",
        "# Get a list of all the files in the photo directory\n",
        "photo_files = os.listdir(\"./kaggle/input/gan-getting-started/photo_jpg\")\n",
        "\n",
        "# Select 20 random Monet images\n",
        "random_monet_files = random.sample(monet_files, 20)\n",
        "\n",
        "# Select 20 random photo images\n",
        "random_photo_files = random.sample(photo_files, 20)\n",
        "\n",
        "# Create a 10x2 grid of subplots\n",
        "fig, axes = plt.subplots(4, 10, figsize=(10, 4))\n",
        "print(\"20 Monet Images above 20 Photos\")\n",
        "\n",
        "# Plot the Monet images in the first column\n",
        "\n",
        "for i, filename in enumerate(random_monet_files):\n",
        "    img = mpimg.imread(os.path.join(\"./kaggle/input/gan-getting-started/monet_jpg\", filename))\n",
        "    axes[math.floor(i%2), math.floor(i/2)].imshow(img)\n",
        "    axes[math.floor(i%2), math.floor(i/2)].axis('off')\n",
        "\n",
        "## Plot the photo images in the second column\n",
        "for i, filename in enumerate(random_photo_files):\n",
        "    img = mpimg.imread(os.path.join(\"./kaggle/input/gan-getting-started/photo_jpg\", filename))\n",
        "    axes[math.floor(i%2+2), math.floor(i/2)].imshow(img)\n",
        "    axes[math.floor(i%2+2), math.floor(i/2)].axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fEOFQxH41eUh"
      },
      "id": "fEOFQxH41eUh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Batch Sizes\n",
        "In order to perform faster data loading, I chose to load the tfrec files.  I then selected a batch size of 30 as that evenly breaks up the dataset into 10 batches.  This will allow for paralell processing across 8 TPU units."
      ],
      "metadata": {
        "id": "8uKwt0Opy1p7"
      },
      "id": "8uKwt0Opy1p7"
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_image(image):\n",
        "  image = tf.image.decode_jpeg(image, channels=3)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = tf.image.resize(image, [256, 256])\n",
        "  return image\n",
        "\n",
        "def read_tfrecord(example):\n",
        "  features = {\n",
        "    'image': tf.io.FixedLenFeature([], tf.string),\n",
        "  }\n",
        "  example = tf.io.parse_single_example(example, features)\n",
        "  image = decode_image(example['image'])\n",
        "  return image\n",
        "tfrecord_dir = './kaggle/input/gan-getting-started/monet_tfrec'\n",
        "tfrecord_files = [os.path.join(tfrecord_dir, f) for f in os.listdir(\n",
        "    tfrecord_dir) if f.endswith('.tfrec')]\n",
        "monet_images_dataset = tf.data.TFRecordDataset(\n",
        "    tfrecord_files).map(read_tfrecord)\n",
        "\n",
        "tfrecord_dir = './kaggle/input/gan-getting-started/photo_tfrec'\n",
        "tfrecord_files = [os.path.join(tfrecord_dir, f) for f in os.listdir(\n",
        "    tfrecord_dir) if f.endswith('.tfrec')]\n",
        "photo_images_dataset = tf.data.TFRecordDataset(\n",
        "    tfrecord_files).map(read_tfrecord)\n",
        "print(monet_images_dataset)\n"
      ],
      "metadata": {
        "id": "N48QEbG-xZRr"
      },
      "id": "N48QEbG-xZRr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: split TFRecordDataset into a train and test set 80/20 split\n",
        "\n",
        "monet_dataset_size = tf.data.experimental.cardinality(\n",
        "    monet_images_dataset).numpy()\n",
        "train_size = int(0.8 * monet_dataset_size)\n",
        "\n",
        "monet_train_dataset = monet_images_dataset.take(train_size)\n",
        "monet_test_dataset = monet_images_dataset.skip(train_size)\n",
        "\n",
        "photo_dataset_size = tf.data.experimental.cardinality(\n",
        "    monet_images_dataset).numpy()\n",
        "train_size = int(0.8 * photo_dataset_size)\n",
        "\n",
        "photo_train_dataset = photo_images_dataset.take(train_size)\n",
        "photo_test_dataset = photo_images_dataset.skip(train_size)"
      ],
      "metadata": {
        "id": "8AWWwlRVH8Dt"
      },
      "id": "8AWWwlRVH8Dt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TPU Setup copied from https://www.tensorflow.org/guide/tpu"
      ],
      "metadata": {
        "id": "4Q1AzzUpzPL3"
      },
      "id": "4Q1AzzUpzPL3"
    },
    {
      "cell_type": "code",
      "source": [
        "resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(resolver)\n",
        "# This is the TPU initialization code that has to be at the beginning.\n",
        "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n"
      ],
      "metadata": {
        "id": "AeRV7dUTzO82"
      },
      "id": "AeRV7dUTzO82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Starting off the GAN\n",
        "I used https://keras.io/examples/generative/cyclegan/ as a reference for the initial model build."
      ],
      "metadata": {
        "id": "fIswnkG0GM5Q"
      },
      "id": "fIswnkG0GM5Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f633f3b8-5a7a-40c6-b90c-6de4c1447ebd",
      "metadata": {
        "id": "f633f3b8-5a7a-40c6-b90c-6de4c1447ebd"
      },
      "outputs": [],
      "source": [
        "img_size = 256\n",
        "image_depth = 3\n",
        "batch_size = 30\n",
        "LR = 0.00012\n",
        "buffer_size = 256"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/examples/generative/cyclegan/\n",
        "# Define the standard image size.\n",
        "orig_img_size = (256, 256)\n",
        "# Size of the random crops to be used during training.\n",
        "input_img_size = (256, 256, 3)\n",
        "# Weights initializer for the layers.\n",
        "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "# Gamma initializer for instance normalization.\n",
        "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
        "\n",
        "\n",
        "def normalize_img(img):\n",
        "    img = tf.cast(img, dtype=tf.float32)\n",
        "    # Map values in the range [-1, 1]\n",
        "    return (img / 127.5) - 1.0\n",
        "\n",
        "\n",
        "def preprocess_train_image(img):\n",
        "    # Random flip\n",
        "    print(img)\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    # Resize to the original size first\n",
        "    img = tf.image.resize(img, [*orig_img_size])\n",
        "    # Random crop to 256X256\n",
        "    img = tf.image.random_crop(img, size=[*input_img_size])\n",
        "    # Normalize the pixel values in the range [-1, 1]\n",
        "    img = normalize_img(img)\n",
        "    return img\n",
        "\n",
        "\n",
        "def preprocess_test_image(img):\n",
        "    # Only resizing and normalization for the test images.\n",
        "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
        "    img = normalize_img(img)\n",
        "    return img"
      ],
      "metadata": {
        "id": "QFG3NTueKiN3"
      },
      "id": "QFG3NTueKiN3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing operations to the training data\n",
        "\n",
        "monet_test_dataset = (\n",
        "    monet_test_dataset.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "monet_train_dataset = (\n",
        "    monet_train_dataset.map(preprocess_train_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "# Apply the preprocessing operations to the test data\n",
        "photo_train_dataset = (\n",
        "    photo_train_dataset.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "photo_test_dataset = (\n",
        "    photo_test_dataset.map(preprocess_test_image, num_parallel_calls=autotune)\n",
        "    .cache()\n",
        "    .shuffle(buffer_size)\n",
        "    .batch(batch_size)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n"
      ],
      "metadata": {
        "id": "srur779GKzvp"
      },
      "id": "srur779GKzvp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReflectionPadding2D(layers.Layer):\n",
        "    \"\"\"Implements Reflection Padding as a layer.\n",
        "\n",
        "    Args:\n",
        "        padding(tuple): Amount of padding for the\n",
        "        spatial dimensions.\n",
        "\n",
        "    Returns:\n",
        "        A padded tensor with the same type as the input tensor.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, padding=(1, 1), **kwargs):\n",
        "        self.padding = tuple(padding)\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, input_tensor, mask=None):\n",
        "        padding_width, padding_height = self.padding\n",
        "        padding_tensor = [\n",
        "            [0, 0],\n",
        "            [padding_height, padding_height],\n",
        "            [padding_width, padding_width],\n",
        "            [0, 0],\n",
        "        ]\n",
        "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
        "\n",
        "\n",
        "def residual_block(\n",
        "    x,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(1, 1),\n",
        "    padding=\"valid\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    dim = x.shape[-1]\n",
        "    input_tensor = x\n",
        "\n",
        "    x = ReflectionPadding2D()(input_tensor)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = activation(x)\n",
        "\n",
        "    x = ReflectionPadding2D()(x)\n",
        "    x = layers.Conv2D(\n",
        "        dim,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.add([input_tensor, x])\n",
        "    return x\n",
        "\n",
        "\n",
        "def downsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_initializer=kernel_init,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        padding=padding,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def upsample(\n",
        "    x,\n",
        "    filters,\n",
        "    activation,\n",
        "    kernel_size=(3, 3),\n",
        "    strides=(2, 2),\n",
        "    padding=\"same\",\n",
        "    kernel_initializer=kernel_init,\n",
        "    gamma_initializer=gamma_init,\n",
        "    use_bias=False,\n",
        "):\n",
        "    x = layers.Conv2DTranspose(\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        strides=strides,\n",
        "        padding=padding,\n",
        "        kernel_initializer=kernel_initializer,\n",
        "        use_bias=use_bias,\n",
        "    )(x)\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    if activation:\n",
        "        x = activation(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "YYC8gsIqLwVs"
      },
      "id": "YYC8gsIqLwVs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_resnet_generator(\n",
        "    filters=64,\n",
        "    num_downsampling_blocks=2,\n",
        "    num_residual_blocks=9,\n",
        "    num_upsample_blocks=2,\n",
        "    gamma_initializer=gamma_init,\n",
        "    name=None,\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
        "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
        "        x\n",
        "    )\n",
        "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    # Downsampling\n",
        "    for _ in range(num_downsampling_blocks):\n",
        "        filters *= 2\n",
        "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Residual blocks\n",
        "    for _ in range(num_residual_blocks):\n",
        "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Upsampling\n",
        "    for _ in range(num_upsample_blocks):\n",
        "        filters //= 2\n",
        "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
        "\n",
        "    # Final block\n",
        "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
        "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
        "    x = layers.Activation(\"tanh\")(x)\n",
        "\n",
        "    model = keras.models.Model(img_input, x, name=name)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "eM_st7BCL0xq"
      },
      "id": "eM_st7BCL0xq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_discriminator(\n",
        "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
        "):\n",
        "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
        "    x = layers.Conv2D(\n",
        "        filters,\n",
        "        (4, 4),\n",
        "        strides=(2, 2),\n",
        "        padding=\"same\",\n",
        "        kernel_initializer=kernel_initializer,\n",
        "    )(img_input)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "    num_filters = filters\n",
        "    for num_downsample_block in range(3):\n",
        "        num_filters *= 2\n",
        "        if num_downsample_block < 2:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(2, 2),\n",
        "            )\n",
        "        else:\n",
        "            x = downsample(\n",
        "                x,\n",
        "                filters=num_filters,\n",
        "                activation=layers.LeakyReLU(0.2),\n",
        "                kernel_size=(4, 4),\n",
        "                strides=(1, 1),\n",
        "            )\n",
        "\n",
        "    x = layers.Conv2D(\n",
        "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
        "    )(x)\n",
        "\n",
        "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Get the generators\n",
        "gen_G = get_resnet_generator(name=\"generator_G\")\n",
        "gen_F = get_resnet_generator(name=\"generator_F\")\n",
        "\n",
        "# Get the discriminators\n",
        "disc_X = get_discriminator(name=\"discriminator_X\")\n",
        "disc_Y = get_discriminator(name=\"discriminator_Y\")\n"
      ],
      "metadata": {
        "id": "_8LNDSTWL4AH"
      },
      "id": "_8LNDSTWL4AH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGan(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        generator_G,\n",
        "        generator_F,\n",
        "        discriminator_X,\n",
        "        discriminator_Y,\n",
        "        lambda_cycle=10.0,\n",
        "        lambda_identity=0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.gen_G = generator_G\n",
        "        self.gen_F = generator_F\n",
        "        self.disc_X = discriminator_X\n",
        "        self.disc_Y = discriminator_Y\n",
        "        self.lambda_cycle = lambda_cycle\n",
        "        self.lambda_identity = lambda_identity\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        gen_G_optimizer,\n",
        "        gen_F_optimizer,\n",
        "        disc_X_optimizer,\n",
        "        disc_Y_optimizer,\n",
        "        gen_loss_fn,\n",
        "        disc_loss_fn,\n",
        "    ):\n",
        "        super().compile()\n",
        "        self.gen_G_optimizer = gen_G_optimizer\n",
        "        self.gen_F_optimizer = gen_F_optimizer\n",
        "        self.disc_X_optimizer = disc_X_optimizer\n",
        "        self.disc_Y_optimizer = disc_Y_optimizer\n",
        "        self.generator_loss_fn = gen_loss_fn\n",
        "        self.discriminator_loss_fn = disc_loss_fn\n",
        "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        self.identity_loss_fn = keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "    def train_step(self, batch_data):\n",
        "        # x is Horse and y is zebra\n",
        "        real_x, real_y = batch_data\n",
        "\n",
        "        # For CycleGAN, we need to calculate different\n",
        "        # kinds of losses for the generators and discriminators.\n",
        "        # We will perform the following steps here:\n",
        "        #\n",
        "        # 1. Pass real images through the generators and get the generated images\n",
        "        # 2. Pass the generated images back to the generators to check if we\n",
        "        #    can predict the original image from the generated image.\n",
        "        # 3. Do an identity mapping of the real images using the generators.\n",
        "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
        "        # 5. Calculate the generators total loss (adversarial + cycle + identity)\n",
        "        # 6. Calculate the discriminators loss\n",
        "        # 7. Update the weights of the generators\n",
        "        # 8. Update the weights of the discriminators\n",
        "        # 9. Return the losses in a dictionary\n",
        "\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            # Horse to fake zebra\n",
        "            fake_y = self.gen_G(real_x, training=True)\n",
        "            # Zebra to fake horse -> y2x\n",
        "            fake_x = self.gen_F(real_y, training=True)\n",
        "\n",
        "            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n",
        "            cycled_x = self.gen_F(fake_y, training=True)\n",
        "            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n",
        "            cycled_y = self.gen_G(fake_x, training=True)\n",
        "\n",
        "            # Identity mapping\n",
        "            same_x = self.gen_F(real_x, training=True)\n",
        "            same_y = self.gen_G(real_y, training=True)\n",
        "\n",
        "            # Discriminator output\n",
        "            disc_real_x = self.disc_X(real_x, training=True)\n",
        "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
        "\n",
        "            disc_real_y = self.disc_Y(real_y, training=True)\n",
        "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
        "\n",
        "            # Generator adversarial loss\n",
        "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
        "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
        "\n",
        "            # Generator cycle loss\n",
        "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
        "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
        "\n",
        "            # Generator identity loss\n",
        "            id_loss_G = (\n",
        "                self.identity_loss_fn(real_y, same_y)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "            id_loss_F = (\n",
        "                self.identity_loss_fn(real_x, same_x)\n",
        "                * self.lambda_cycle\n",
        "                * self.lambda_identity\n",
        "            )\n",
        "\n",
        "            # Total generator loss\n",
        "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
        "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
        "\n",
        "            # Discriminator loss\n",
        "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
        "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
        "\n",
        "        # Get the gradients for the generators\n",
        "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
        "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
        "\n",
        "        # Get the gradients for the discriminators\n",
        "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
        "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
        "\n",
        "        # Update the weights of the generators\n",
        "        print(type(grads_G))\n",
        "        print(type(self.gen_G.trainable_variables))\n",
        "        self.gen_G_optimizer.apply_gradients(\n",
        "            zip(grads_G, self.gen_G.trainable_variables)\n",
        "        )\n",
        "        self.gen_F_optimizer.apply_gradients(\n",
        "            zip(grads_F, self.gen_F.trainable_variables)\n",
        "        )\n",
        "\n",
        "        # Update the weights of the discriminators\n",
        "        self.disc_X_optimizer.apply_gradients(\n",
        "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
        "        )\n",
        "        self.disc_Y_optimizer.apply_gradients(\n",
        "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"G_loss\": total_loss_G,\n",
        "            \"F_loss\": total_loss_F,\n",
        "            \"D_X_loss\": disc_X_loss,\n",
        "            \"D_Y_loss\": disc_Y_loss,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "u5MsfOjeL434"
      },
      "id": "u5MsfOjeL434",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "I3xKpMnzL3oC"
      },
      "id": "I3xKpMnzL3oC"
    },
    {
      "cell_type": "code",
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate and save images after each epoch\"\"\"\n",
        "\n",
        "    def __init__(self, num_img=4):\n",
        "        self.num_img = num_img\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        _, ax = plt.subplots(4, 2, figsize=(12, 12))\n",
        "        for i, img in enumerate(test_horses.take(self.num_img)):\n",
        "            prediction = self.model.gen_G(img)[0].numpy()\n",
        "            prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "            img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "            ax[i, 0].imshow(img)\n",
        "            ax[i, 1].imshow(prediction)\n",
        "            ax[i, 0].set_title(\"Input image\")\n",
        "            ax[i, 1].set_title(\"Translated image\")\n",
        "            ax[i, 0].axis(\"off\")\n",
        "            ax[i, 1].axis(\"off\")\n",
        "\n",
        "            prediction = keras.utils.array_to_img(prediction)\n",
        "            prediction.save(\n",
        "                \"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch + 1)\n",
        "            )\n",
        "        plt.show()\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "MQWl-RNhL3Pj"
      },
      "id": "MQWl-RNhL3Pj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function for evaluating adversarial loss\n",
        "adv_loss_fn = keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "# Define the loss function for the generators\n",
        "def generator_loss_fn(fake):\n",
        "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
        "    return tf.reduce_mean(fake_loss)\n",
        "\n",
        "# Define the loss function for the discriminators\n",
        "def discriminator_loss_fn(real, fake):\n",
        "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
        "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
        "    return (real_loss + fake_loss) * 0.5\n",
        "\n",
        "# Assuming you have already defined your strategy\n",
        "strategy = tf.distribute.TPUStrategy(resolver)\n",
        "print(strategy)\n",
        "with strategy.scope():\n",
        "    # Create cycle gan model\n",
        "    cycle_gan_model = CycleGan(\n",
        "        generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
        "    )\n",
        "\n",
        "    # Compile the model\n",
        "    cycle_gan_model.compile(\n",
        "        gen_G_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "        gen_F_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "        disc_X_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "        disc_Y_optimizer=tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
        "        gen_loss_fn=generator_loss_fn,\n",
        "        disc_loss_fn=discriminator_loss_fn\n",
        "    )\n",
        "\n",
        "    # Callbacks for checkpointing\n",
        "    checkpoint = tf.train.Checkpoint(cycle_gan_model=cycle_gan_model)\n",
        "    checkpoint_manager = tf.train.CheckpointManager(checkpoint, directory='./checkpoints', max_to_keep=5)\n",
        "\n",
        "    # Restore the latest checkpoint if it exists\n",
        "    if checkpoint_manager.latest_checkpoint:\n",
        "        checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
        "        print(f'Restored from {checkpoint_manager.latest_checkpoint}')\n",
        "    else:\n",
        "        print('Starting training from scratch')\n",
        "\n",
        "# Define your training dataset\n",
        "train_dataset = tf.data.Dataset.zip((monet_train_dataset, photo_train_dataset))\n",
        "\n",
        "# Train the model manually and save checkpoints\n",
        "EPOCHS = 10  # Number of epochs you want to train\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Starting epoch {epoch+1}/{EPOCHS}')\n",
        "\n",
        "    # Train the model for one epoch\n",
        "    cycle_gan_model.fit(train_dataset, epochs=1)\n",
        "\n",
        "    # Save a checkpoint after each epoch\n",
        "    save_path = checkpoint_manager.save()\n",
        "    print(f'Saved checkpoint for epoch {epoch+1}: {save_path}')"
      ],
      "metadata": {
        "id": "RIFd5rFhL_wH"
      },
      "id": "RIFd5rFhL_wH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!curl -LO https://github.com/AakashKumarNain/CycleGAN_TF2/releases/download/v1.0/saved_checkpoints.zip\n",
        "#!unzip -qq saved_checkpoints.zip\n"
      ],
      "metadata": {
        "id": "wo4JOteHMNei"
      },
      "id": "wo4JOteHMNei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the checkpoints\n",
        "weight_file = \"./saved_checkpoints/cyclegan_checkpoints.090\"\n",
        "cycle_gan_model.load_weights(weight_file).expect_partial()\n",
        "print(\"Weights loaded successfully\")\n",
        "\n",
        "_, ax = plt.subplots(4, 2, figsize=(10, 15))\n",
        "for i, img in enumerate(photo_train_dataset.take(4)):\n",
        "    prediction = cycle_gan_model.gen_G(img, training=False)[0].numpy()\n",
        "    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n",
        "    img = (img[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n",
        "\n",
        "    ax[i, 0].imshow(img)\n",
        "    ax[i, 1].imshow(prediction)\n",
        "    ax[i, 0].set_title(\"Input image\")\n",
        "    ax[i, 0].set_title(\"Input image\")\n",
        "    ax[i, 1].set_title(\"Translated image\")\n",
        "    ax[i, 0].axis(\"off\")\n",
        "    ax[i, 1].axis(\"off\")\n",
        "\n",
        "    prediction = keras.utils.array_to_img(prediction)\n",
        "    prediction.save(\"predicted_img_{i}.png\".format(i=i))\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "92luZ7luMB2x"
      },
      "id": "92luZ7luMB2x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BiVJoh35L6Y9"
      },
      "id": "BiVJoh35L6Y9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc1"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}